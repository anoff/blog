<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Andreas Offenhaeuser"><meta name=description content="Andreas' personal blog"><meta name=keywords content=blog,personal,software,architecture,node,markdown,plantuml,developer><meta name=generator content="Hugo 0.55.6"><title>terraform | Andreas&#39; Blog</title><meta name=description content="terraform - Andreas' personal blog"><meta itemprop=name content=terraform><meta itemprop=description content="terraform - Andreas' personal blog"><meta property=og:title content=terraform><meta property=og:description content="terraform - Andreas' personal blog"><meta property=og:image content="https://www.gravatar.com/avatar/d41d8cd98f00b204e9800998ecf8427e?size=200"><meta property=og:url content=https://blog.anoff.io/tags/terraform/><meta property=og:site_name content="Andreas' Blog"><meta property=og:type content=website><link rel=icon type=image/png href=https://blog.anoff.io/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://blog.anoff.io/favicon-16x16.png sizes=16x16><link href=/tags/terraform/ rel=alternate type=application/rss+xml title="Andreas' Blog"><link href=/tags/terraform/ rel=feed type=application/rss+xml title="Andreas' Blog"><link rel=stylesheet href=/sass/combined.min.52f29c7c50b8b42d07e557633ac1b4544e3f91c2f3a5bb0ab34dcd0e14b4e38e.css><link rel=stylesheet href=https://blog.anoff.io/adoc.css><link rel=stylesheet href=https://blog.anoff.io/overwrites.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.css></head><body class=bilberry-hugo-theme><nav class=permanentTopNav><div class=container><ul class=topnav><li><a href=//anoff.io/ target=_self>About me</a></li><li><a href=/ target=_self>Blog</a></li><li><a href=//anoff.io/project/ target=_self>Projects</a></li><li><a href=/tags target=_self>by Tags</a></li><li><a href=//radar.anoff.io target=_blank>Tech skills</a></li><li><a href=//anoff.github.io/legal/ target=_blank>Legal</a></li></ul></div></nav><header><div class=container><div class=logo><a href=/ class=logo><img src=/avatar.png alt>
<span class=overlay><i class="fa fa-child"></i></span></a></div><div class=titles><h3 class=title><a href=/>Andreas&#39; Blog</a></h3><span class=subtitle>Adventures of a software engineer/architect</span></div><div class="toggler permanentTopNav"><i class="fa fa-bars" aria-hidden=true></i></div></div></header><div class="main container"><div class="article-wrapper u-cf"><a class=bubble href=https://blog.anoff.io/2018-03-22-dev-workflow-datascience/><i class="fa fa-fw fa-code"></i></a><article class="default article"><div class=content><h3><a href=https://blog.anoff.io/2018-03-22-dev-workflow-datascience/>Automated dev workflow for using Data Science VM on Azure</a></h3><div class=meta><span class="date moment">2018-03-22</span>
<span class=readingTime>11 min read</span>
<span class=author><a href=/author/anoff>anoff</a></span></div><blockquote><p>tl;dr; I put together a bunch of scripts on Github that let you deploy a VM from your command line as well as sync code from your local directory to the VM easily to be able to use local IDE and git but execute on the powerful remote machine. Perfect for Data Science applications based around jupyter notebook.</p></blockquote><p>In my <a href=./2018-01-23-dsvm-terraform.md>previous blog post</a> I explained how to do <a href=https://www.terraform.io/intro/index.html>Terraform</a> deployment of an <a href=https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/overview>Azure Data Science Virtual Machine</a>.</p><p><img src=/assets/datascience-dev/logo.png alt="Overview of available commands"></p><h1 id=motivation>Motivation üòì</h1><p>Recently I started to do some #deeplearning üîÆ as part of my Udacity Artificial Intelligence Nanodegree. When I was working on the #deeplearning Nanodegree last year I started to <a href=https://github.com/anoff/nd101/blob/master/gan_mnist/Makefile>script starting/stopping an AWS GPU VM</a> and rsyncing code around. This time I felt like giving the Azure Cloud a try. Mostly because my daytime job lets me look at a lot of their services I wanted to venture deeper into the Azure Data Science offerings. Being more of a software developer by trait and less of a data scientist üë®‚Äçüî¨ I often feel like my standards for versioning, testing and ease of development are beyond those that the ML ecosystem offers by default (hope that doesn‚Äôt offend the data wizards out there). My development machine is a small MacBook without GPU support. So to train neural networks I wanted to get a Virtual Machine with a GPU on board. Azure offers VMs with a <a href=https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/>prebaked Ubuntu image</a> containing all of today&rsquo;s Data Science tools: Python, Conda, Jupyter, GPU Neuralnet libs etc.</p><ul><li>Microsoft R Server 9.2.1 with Microsoft R Open 3.4.1</li><li>Anaconda Python 2.7 and 3.5</li><li>JupyterHub with sample notebooks
Spark local 2.2.0 with PySpark and SparkR - Jupyter kernels</li><li>Single node local Hadoop</li><li>Azure command-line interface
Visual Studio Code, IntelliJ IDEA, PyCharm, - and Atom</li><li>H2O, Deep Water, and Sparkling Water</li><li>Julia</li><li>Vowpal Wabbit for online learning</li><li>xgboost for gradient boosting</li><li>SQL Server 2017</li><li>Intel Math Kernel Library</li></ul><p>See the full stack available on DSVM on the <a href=https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/overview>Azure docs</a></p><p>Having the perfect target for running my code I was wondering how to actually keep my local machine my main development machine‚Ää‚Äî‚Äämeaning I don‚Äôt want to setup git on the VM to version my code. This is where our friend <a href=https://en.wikipedia.org/wiki/Rsync>rsync</a> comes into the picture üñº. It lets you sync two directories over SSH.</p><pre><code class=language-text>rsync [OPTION]... SRC [SRC]... [USER@]HOST::DEST
</code></pre><h1 id=the-goal>The Goal üèÅ</h1><p>Being prone to over-engineering üôÑ my side projects I started my journey of automating my VM workflow with the following goals:</p><ol><li>Deploy (and delete) an entire VM by a template that I can version on Github</li><li>Start/Stop the VM from my command line so I don‚Äôt pay for it if I don‚Äôt need it (GPU VMs are üí∞üí∞üí∞)</li><li>Get code changes I make on the VM using jupyter notebook synchronized to my local machine so I can git commit</li></ol><h1 id=deploy-infrastructure>Deploy infrastructure üì¶</h1><p>Again I opted for Terraform to deploy the VM. As mentioned in my previous blog post you could also use <a href=https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-overview>Azure Resource Manager Templates</a> for that, but my personal favorite is <a href=https://www.terraform.io/>Terraform</a> üòç. So I continued from my previous findings to build the following Terraform recipe. The suggested setup is to place the script into an infra folder into your projects working directory.</p><p><a href=https://github.com/anoff/vm-automation/blob/master/azure_dsvm.tf><strong>anoff/vm-automation</strong> -
Bootstrap a VM for machine learning applicationsgithub.com</a></p><p>It creates several resources:</p><ul><li>resource group: <em>logical grouping on Azure that contains all the resources below</em></li><li>virtual network: <em>well..a virtual private network that your - resources use to communicate*</em>
network subnet: <em>the subnet that your VPN will use*</em></li><li>network interface: <em>a network interface so your VM can bind against the virtual network</em></li><li>virtual machine: <em>the actual compute resource (will spawn disk resources for file system)</em></li><li>public IP address: <em>a static IP that will make your VM reachable from the internet</em></li><li>local executor (null resource): <em>used to write some results of the VM creation process onto your disk</em></li></ul><p><code>*</code> sorry if I did not explain those correctly tbh I am not üíØ% sure I understand correctly what they do either üòä</p><p><img src=/assets/datascience-dev/rg.png alt="Screenshot of Azure Portal"></p><blockquote><p>These are the created resources</p></blockquote><pre><code class=language-HCL>variable &quot;location&quot; {
  description = &quot;Datacenter location to deploy the VM into&quot;
  default     = &quot;westeurope&quot;
}

variable &quot;vm_name&quot; {
  description = &quot;Name of the virtual machine (acts as prefix for all generated resources)&quot;
  default     = &quot;dsvm&quot;
}

variable &quot;admin_user&quot; {
  description = &quot;Admin username&quot;
  default     = &quot;root&quot;
}

variable &quot;admin_public_key&quot; {
  description = &quot;Path to Public SSH key of the admin user&quot;
  default     = &quot;~/.ssh/id_rsa.pub&quot;
}

variable &quot;admin_private_key&quot; {
  description = &quot;Path to Private SSH key of the admin user&quot;
  default     = &quot;~/.ssh/id_rsa&quot;
}

variable &quot;vm_type&quot; {
  description = &quot;The type of VM to deploy&quot;
  default     = &quot;Standard_NC6&quot;
}
</code></pre><blockquote><p>Variables in the Terraform recipe</p></blockquote><p>Leveraging <a href=https://www.terraform.io/docs/configuration/variables.html>Terraform variables</a> some of the properties of this recipe can be customized.</p><p>The easiest way to change the variable values is to config.auto.tfvars file that contains all the variable names and their description as well.</p><p>You can find it in the <a href=https://github.com/anoff/vm-automation/blob/master/config.auto.tfvars>Github repo</a> right next to the Terraform recipe itself. As you can see all variables have a default value even if you not specify the .tfvars properties. The ones you most likely want to modify are admin_public_key and admin_private_key</p><p>They are a <a href=https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/>SSH key pair</a> that you will use to authenticate when connecting to the virtual machine later. During the Terraform process the public key will be stored on the VM so it will later recognize it as a valid key. The private key will be used to SSH into the machine near the end of the process to actually prepare the local file system for later file transfers‚Ää‚Äî‚Äänamely create a ~/work directory. You might also want to modify the admin username or the resource location.</p><pre><code class=language-sh># Path to Public SSH key of the admin user (required)
admin_public_key = &quot;~/.ssh/id_rsa.pub&quot;

# Path to Private SSH key of the admin user (required)
admin_private_key = &quot;~/.ssh/id_rsa&quot;

# Datacenter location to deploy the VM into (default: westeurope)
#location    = &quot;westeurope&quot;

# Name of the virtual machine (acts as prefix for all generated resources, default: dsvm)&quot;
#vm_name     = &quot;myspecialvm&quot;

# Admin username (default: root)
admin_user = &quot;andreas&quot;

# Type of VM to deploy (default: Standard_NC6 - GPU instance)
#vm_type = &quot;Standard_A1_V2&quot;
</code></pre><blockquote><p>config.auto.tfvars</p></blockquote><h1 id=signing-the-license-terms-for-data-science-vm>Signing the license terms for Data Science VM ‚öñÔ∏è</h1><p>You might see the following error when trying to run the Terraform script without having read this far.</p><p><img src=/assets/datascience-dev/terms.png alt="Error during applying plan"></p><blockquote><p>Terraform error due to missing license agreement</p></blockquote><p>The problem is that the DSVM is published via the Azure market place and even though it does not incur additional charges on top of the Azure VM resources you need to read and agree to the license terms. You can do this as described in the error message via Powershell. The complete process of opening a Powershell is explained in this <a href=https://github.com/anoff/vm-automation#sign-the-terms-of-service-%EF%B8%8F>Readme</a>. The short version if you already have Powershell open is to run:</p><pre><code class=language-sh># Use this command to view the current license agreement

$ Get-AzureRmMarketplaceTerms -Publisher &quot;microsoft-ads&quot; -Product &quot;linux-data-science-vm-ubuntu&quot; -Name &quot;linuxdsvmubuntu&quot;

# If you feel confident to agree to the agreement use the following command to enable the offering for your subscription

$ Get-AzureRmMarketplaceTerms -Publisher &quot;microsoft-ads&quot; -Product &quot;linux-data-science-vm-ubuntu&quot; -Name &quot;linuxdsvmubuntu&quot; | Set-AzureRmMarketplaceTerms -Accept
</code></pre><p>After successfully signing the license terms you should see the following output in your shell</p><p><img src=/assets/datascience-dev/powershell_out.png alt="Accepted Terms of service in Powershell"></p><h1 id=run-terraform>Run Terraform üèÉ‚Äç‚ôÇÔ∏è</h1><p>Once the license terms are signed you can initialize Terraform using terraform init and then can run terraform apply to bring up the infrastructure resources on azure. It may take up 5~10 minutes to fully provision the virtual machine.</p><p>After running it you may notice two new files being created. Both contain a link to the created virtual machine. .vm-ip contains the public IP address that was created and will be used to SSH into the machine. .vm-id is the Azure Resource ID of your virtual machine and is a unique identifier that we will use to start/stop the machine later. Both are plain text files and only contain one line, feel free to check them out. The machine is now up and running and you can work with it.</p><h1 id=bring-code-onto-the-vm>Bring code onto the VM üíÅ‚Äç</h1><p>Before doing any work you might want to upload some bootstrap code onto the virtual machine‚Ää‚Äî‚Ääor you just want to run an existing jupyter notebook there. Again the Github repository holds a small script that will help you do this (works out of the box on Mac/Unix machines otherwise you need to install make and rsync first).</p><p>Place the <a href=https://github.com/anoff/vm-automation/blob/master/Makefile>Makefile</a> into the working directory of your code and make sure to update the <code>PATH</code> definitions to the two files mentioned at the end of the last chapter containing the details of the newly created virtual machine. If you had the Terraform script in a subfolder named infra there is nothing to do. Otherwise you should either copy over the two files into such a directory or modify the <code>PATH</code> definition in the Makefile.</p><p>Use make syncup in your working directory (where you placed the Makefile) to sync your local directory content onto the remote machine. You can see the command that is being executed and what the remote directory will be named. In my case it is <code>~/work/AIND-RNN</code> which is one of my Nanodegree projects. You can also see that the command automatically ignores all files that are defined in your .gitignore which means anything you do not want to version will also not be copied around. This is especially useful for artifacts created during neural net training processes.</p><p><img src=/assets/datascience-dev/syncup.png alt="Running make syncup"></p><blockquote><p>Output of make syncup</p></blockquote><h1 id=run-jupyter-notebook>Run Jupyter Notebook üìí</h1><p>Let‚Äôs assume that your project also holds a Jupyter notebook you want to execute on the remote machine and access from your local browser. <em>You could also use a similar process to execute ANY kind of script on the remote machine.</em></p><p>First you need to SSH into the machine using make ssh which will also do port forwarding for the Jupyter Port <strong>8888</strong> onto your local machine so you can open <a href=http://localhost:8888>http://localhost:8888</a> in your local browser (my MacBook) and connect to the webserver that listens on this port on the virtual machine (Jupyter Notebook). Now that you have a shell running on the DSVM manipulate the file system, install missing packages via pip/conda or just start a process.</p><p><img src=/assets/datascience-dev/jupyter.png alt="Jupyter logs"></p><blockquote><p>Starting jupyter notebook on the VM</p></blockquote><p>The Jupyter notebook process started above is linked to the lifecycle of the interactive shell that we opened with the SSH connection. Closing the SSH connection will kill the Jupyter server as well. <em>All your code should still be there as Jupyter regularly saves to disk but your python kernel will be gone and all the memory objects (state of notebook execution) will be lost. <strong>You will need to execute the notebook again from beginning after</strong> you SSH again into the machine and start Jupyter up.</em></p><h1 id=commit-your-changes>Commit your changes üíæ</h1><p>After you did some changes and you want to git commit like a good developer you need to get those changes you did on the virtual machine to your local development environment. You can do this using make syncdown which will copy all changed remote files onto your local working directory‚Ää‚Äî‚Ääagain only those under git version control.</p><blockquote><p><strong>üö®Make sure you exit the SSH connection first</strong></p></blockquote><p><img src=/assets/datascience-dev/syncdown.png alt="Running make syncdow"></p><blockquote><p>Copy remote changes to local filesystem</p></blockquote><p>The remote file <code>LOOK_MOMMY_I_CHANGED_A_FILE</code> has now been copied to my local working directory and I can use <code>git commit -am &quot;everyone uses meaningful commit messages right?&quot;</code> to commit my changes or use my local tooling to execute unit tests, check codestyle, add some comments‚Ä¶</p><h1 id=start-and-stop-the-virtual-machine>Start and Stop the Virtual Machine üé¨ üõë</h1><p>If you have not checked already, you should look up how much the Virtual Machine that you provisioned actually costs you. The <strong>Standard_NC6</strong> (which is the cheapest GPU instance) will cost you a small holiday if you keep it running for a month. That is the reason why I wanted an easy way to stop it when I don‚Äôt need it and get it back up quickly if I want to continue working.</p><p><img src=/assets/datascience-dev/vm_prices.png alt="Azure prices for GPU VMs"></p><p>The Makefile comes with three commands for managing the state of the virtual machine itself. They all require the unique Azure Resource ID located in the <code>.vm-id</code> to select the correct VM in your Azure subscription:</p><p>make stop will stop stop the virtual machine AND deallocate the resources which will significantly reduce the costs as you only pay for the disks that hold your data.</p><p>make start tells Azure to allocate new resources and spawn up the virtual machine again</p><p>make status will tell you if the virtual machine is up or not</p><p><img src=/assets/datascience-dev/start.png alt="Running make start/stop"></p><blockquote><p>Virtual Machine start/status/stop</p></blockquote><p>The screenshot shows you how long stopping and starting the VM might take. However as soon as you see the CLI saying <code>Running</code> you can shut down your local machine as Azure started deallocating your resources.</p><h1 id=reduce-risk-of-bankruptcy>Reduce risk of bankruptcy üí∏</h1><p>If you are afraid of the bill that might come flying in if you miss stopping the virtual machine you should take a closer look at the <strong>auto shutdown</strong> features that Azure offers you. It lets you specify a time at which the VM will automatically shut down every day.</p><p><img src=/assets/datascience-dev/auto_shutdown_menu.png alt></p><p><img src=/assets/datascience-dev/auto_shutdown.png alt="Activate auto shutdown"></p><blockquote><p>But let me tell you from experience‚Ää‚Äî‚Ääif you accidentally keep it up for a weekend and see the bill the next week you will <em>always</em> shutdown from then on. That was actually one of the reasons why I wanted to make this workflow as easy as possible.</p></blockquote><h1 id=summary>Summary üìö</h1><p>Well I hope you liked this article and found some helpful tips. The general workflow can also be done with AWS machines but the Terraform template will look different. Feel free to submit a PR to my Repo and I will add the option to also use AWS resources.</p><p>I would love to hear feedback via <a href=https://github.com/anoff/vm-automation/issues/new>Issues</a>, <a href=https://twitter.com/anoff_io>Twitter üê¶</a> or comments. One thought I have is to bundle all the commands into a binary CLI so it works cross platform and can just be installed by copying around a single file. If you‚Äôre interested please let me knowüòª</p><p>Here is another look at all the commands you can useüßô‚Äç‚ôÄÔ∏è</p><p><img src=/assets/datascience-dev/logo.png alt=Overview></p><p>/andreas</p></div><div class=footer><div class=tags><i class="fa fa-tags"></i><div class=links><a href=/tags/datascience>datascience</a>
<a href=/tags/terraform>terraform</a>
<a href=/tags/azure>azure</a></div></div></div></article></div><div class="article-wrapper u-cf"><a class=bubble href=https://blog.anoff.io/2018-01-23-dsvm-terraform/><i class="fa fa-fw fa-pencil"></i></a><article class="default article"><div class=content><h3><a href=https://blog.anoff.io/2018-01-23-dsvm-terraform/>Deploy Datascience infrastructure on Azure using Terraform</a></h3><div class=meta><span class="date moment">2018-01-23</span>
<span class=readingTime>6 min read</span>
<span class=author><a href=/author/anoff>anoff</a></span></div><p>In this article I will talk about my experience building my first infrastructure deployment using Terraform that does (a little) more than combining off-the-shelf resources.</p><h1 id=the-stack-we-will-deploy>The stack we will deploy üì¶</h1><p>Lately I‚Äôve been looking at a lot of Microsoft Azure services in the big data area. I am looking for something to replace a Hadoop based üêò data analytics environment consisting mainly of HDFS, Spark &amp; Jupyter.</p><p><img src=/assets/terraform-dsvm/logo.png alt="How to Datascience on Azure?"></p><p>The most obvious solution is to use a <a href=https://azure.microsoft.com/en-us/services/hdinsight/>HDInsight cluster</a> which is basically a managed Hadoop that you can pick in different flavours. However with the elasticity of the cloud at my hands I wanted to go for a more diverse setup that also allows a pure #python üêç based data science stack without the need to use Spark. One reason for this is that many use cases do not require a calculation on all of the data but just use spark to mask the data. For the actual analysis/training the amount of data often fits in RAM‚Ää‚Äî‚Ääif I get a bit more than my MacBook Pro has to offer üôÑ. The solution described in this article consists of a <a href=https://docs.microsoft.com/en-us/azure/machine-learning/data-science-virtual-machine/overview>Data Science Virtual Machine</a> üñ• and <a href=https://docs.microsoft.com/en-us/azure/storage/files/storage-files-introduction>Azure Files</a> üìÑ for common data store. As the file storage account has a cap on 5TB you might need something different if you really have a lot of data‚Ää‚Äî‚Ääor use multiple fileshares.</p><p><img src=/assets/terraform-dsvm/shared_storage.png alt="Multiple VMs accessing a shared storage"></p><blockquote><p>Target setup with multiple data scientist VM accessing a common data pool</p></blockquote><h1 id=quick-intro-to-terraform>Quick intro to Terraform üëÄ</h1><blockquote><p>Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. [‚Ä¶] Terraform generates an execution plan describing what it will do to reach the desired state, and then executes it to build the described infrastructure. As the configuration changes, Terraform is able to <strong>determine what changed and create incremental execution plans</strong> which can be applied.</p></blockquote><p><a href=https://en.wikipedia.org/wiki/Infrastructure_as_Code>Infrastructure as Code (IaC)</a> is an important aspect for me when managing a cloud solution. I want to be able to automate ‚öôÔ∏è everything from infrastructure provisioning to code deployment. Amazon Web Services has Cloudformation, Azure has the Azure Resource Manager (ARM) templates but <a href=https://www.terraform.io/intro/index.html>Hashicorps Terraform</a> provides a somewhat cloud agnostic layer on top of those proprietary tools. &lsquo;<em>Somewhat</em>&rsquo; because the recipes you write are specific to a certain cloud environment but it allows you to use common syntax to deploy a multi-cloud solution. The main reason for me to use Terraform instead of ARM is that it offers a better way to create modular recipes and not worry about handling the state during incremental deployments.</p><h1 id=building-the-terraform-recipe>Building the Terraform recipe üìú</h1><p>When deploying any higher level components the first thing to do is figure out what underlying infrastructure is used. The easiest way to do this is click yourself through the Portal to create the resource‚Ää‚Äî‚Ääin this case the Data Scientist VM‚Ää‚Äî‚Ääand look at the ARM template that drops out of this process. You could use it to deploy this VM automatically using either the ARM tooling or the <a href=https://www.terraform.io/docs/providers/azurerm/r/template_deployment.html>azurerm_template_deployment</a> in Terraform. However the ARM templates are way too complex to maintain to my liking.</p><p><img src=/assets/terraform-dsvm/portal_deploy.png alt="Access ARM template from portal"></p><p><img src=/assets/terraform-dsvm/arm_view.png alt="Overview of ARM resources"></p><blockquote><p>Resource view of the ARM template in Azure Portal</p></blockquote><p>In the case of the Data Scientist VM you can see that five different resources are deployed to bring up an Azure VM. The machine itself which consists of compute and memory allocations. A storage account that holds the disk images and a couple of networking components.</p><p>If you look at the <a href=https://www.terraform.io/docs/providers/azurerm/r/virtual_machine.html>Virtual Machine recipe that is available on the Terraform docs</a> you may see similar components. There they are called _azurerm_virtual<em>network</em>, _azurerm<em>subnet</em>, _azurerm_network<em>interface</em>, _azurerm_virtual<em>machine</em>. I took the Terraform example as a base for my recipe and tried to modify this vanilla Ubuntu VM into the Data Science VM that I clicked together using the Portal. So I was mostly interested in figuring out what part of the VM deployment makes the VM a Data Science VM with all those fancy software packages pre-installed.</p><pre><code class=language-json>  &quot;resources&quot;: [
  {
      &quot;name&quot;: &quot;[parameters('virtualMachineName')]&quot;,
      &quot;type&quot;: &quot;Microsoft.Compute/virtualMachines&quot;,
      &quot;apiVersion&quot;: &quot;2016-04-30-preview&quot;,
      &quot;location&quot;: &quot;[parameters('location')]&quot;,
      &quot;dependsOn&quot;: [
          &quot;[concat('Microsoft.Network/networkInterfaces/', parameters('networkInterfaceName'))]&quot;,
          &quot;[concat('Microsoft.Storage/storageAccounts/', parameters('diagnosticsStorageAccountName'))]&quot;
      ],
      &quot;properties&quot;: {
          &quot;osProfile&quot;: {
              &quot;computerName&quot;: &quot;[parameters('virtualMachineName')]&quot;,
              &quot;adminUsername&quot;: &quot;[parameters('adminUsername')]&quot;,
              &quot;adminPassword&quot;: &quot;[parameters('adminPassword')]&quot;
          },
          &quot;hardwareProfile&quot;: {
              &quot;vmSize&quot;: &quot;[parameters('virtualMachineSize')]&quot;
          },
          &quot;storageProfile&quot;: {
              &quot;imageReference&quot;: {
                  &quot;publisher&quot;: &quot;microsoft-ads&quot;,
                  &quot;offer&quot;: &quot;linux-data-science-vm-ubuntu&quot;,
                  &quot;sku&quot;: &quot;linuxdsvmubuntu&quot;,
                  &quot;version&quot;: &quot;latest&quot;
              },
              &quot;osDisk&quot;: {
                  &quot;createOption&quot;: &quot;fromImage&quot;,
                  &quot;managedDisk&quot;: {
                      &quot;storageAccountType&quot;: &quot;Premium_LRS&quot;
                  }
              },
              &quot;dataDisks&quot;: [
                  {
                      &quot;createOption&quot;: &quot;fromImage&quot;,
                      &quot;lun&quot;: 0,
                      &quot;managedDisk&quot;: {
                          &quot;storageAccountType&quot;: &quot;Premium_LRS&quot;
                      }
                  }
              ]
          }
      },
      &quot;plan&quot;: {
          &quot;name&quot;: &quot;linuxdsvmubuntu&quot;,
          &quot;publisher&quot;: &quot;microsoft-ads&quot;,
          &quot;product&quot;: &quot;linux-data-science-vm-ubuntu&quot;
      }
  }
</code></pre><blockquote><p>The part of the ARM template that specifies the OS setup</p></blockquote><p>The important fields to look at are the way the <strong>imagReference</strong>, <strong>osDisk</strong> and <strong>dataDisks</strong> are created and the <strong>plan</strong> that is required if you want to deploy a marketplace VM. These differ from the vanilla setup that we get from the Terraform documentation. By going through the Terraform docs on the VM provider you can identify the fields necessary to turn the example VM into a Data Science VM. The main changes are to create a storage_data_disk that has the create_option = fromImageThis seems to be required as the DSVM ships with some data according to the ARM template. The second thing to add is the plan property into your VM recipe. This should be set with the same parameters as shown above in the ARM snippet.</p><p>You can find my final resulting code <a href=https://github.com/anoff/tf-azure-datascience/blob/8eff92fd4c8e609f6f938fe4230fcc940a1783d0/ds-vm.tf#L31>on github</a> üëØ‚Äç</p><h1 id=that-s-it-for-the-data-scientist-vm-now-on-to-the-file-share>That‚Äòs it for the Data Scientist VM, now on to the File share üìÑ</h1><p>Once you understand that the File service on Azure is part of the Storage suite you can either follow along the example above and look at the ARM template that the Portal generates or hop right into the <a href=https://www.terraform.io/docs/providers/azurerm/r/storage_share.html>Terraform documentation</a> and look for possible ways to deploy storage resources.</p><p>_Note: Before writing this article I didn‚Äôt realize there is an option to create a Fileshare using Terraform. So I initially built a <a href=https://github.com/anoff/tf-azure-datascience/blob/8eff92fd4c8e609f6f938fe4230fcc940a1783d0/provision_fileshare>custom Azure CLI script</a> and hooked that into the storage recipe. Take a look at the <a href=https://github.com/anoff/tf-azure-datascience/blob/8eff92fd4c8e609f6f938fe4230fcc940a1783d0/storage.tf#L15>code history</a> if you want to learn more about fusing Terraform with the Azure CLI._</p><p><img src=/assets/terraform-dsvm/tf_storage.png alt="Terraform resources for storage"></p><blockquote><p>Storage resources in the Azure Provider for Terraform</p></blockquote><p>Creating a recipe for the Fileshare is literally just copying the <a href=https://www.terraform.io/docs/providers/azurerm/r/storage_share.html#example-usage>example</a> as it does not have any customised properties. Make sure you give the whole setup pretty names and the correct quote and you‚Äôre done.</p><p><img src=/assets/terraform-dsvm/terraform_out.png alt="Terraform output during creation"></p><p>Run <code>terraform apply -auto-approve</code> to execute the recipe. In my latest run it took 2min 42sto spin up all the required resources.</p><p>Killing the components took twice as long üôÑ</p><p>The full recipe will provision the following 6 resources for you. You might notice that Terraform mentions 7 added resources, the difference of 1 comes from the resource group that is not listed below. If you want to clean up just run <code>terraform destroy</code>.</p><p><img src=/assets/terraform-dsvm/azure_rg.png alt="Azure resource group overview"></p><blockquote><p>Fully provisioned Datascience Setup</p></blockquote><h1 id=next-steps>Next steps üëü</h1><p>There‚Äôs a bunch of things I want to improve on this setup:</p><ol><li><p>Create a Terraform module for the Virtual Machine setup to easily create multiple VMs without cloning the 50+ lines recipe. The goal would be that the name of the VM, the size and the network can be defined. Ideally multiple data scientists would work in a common network.</p></li><li><p>Auto-mount the file share into the VMs during creation. The <a href=https://www.terraform.io/docs/provisioners/remote-exec.html>remote-exec provisioner</a> might be a good way to start.</p></li></ol><p>Feel free to discuss this approach or even implement improvements for this via a PR on github.com/anoff/tf-azure-datascience or on <a href=https://twitter.com/anoff_io>twitter üê¶</a>. Hope you enjoyed my first article üï∫.</p></div><div class=footer><div class=tags><i class="fa fa-tags"></i><div class=links><a href=/tags/terraform>terraform</a>
<a href=/tags/azure>azure</a>
<a href=/tags/datascience>datascience</a></div></div></div></article></div><div class=paginator></div></div><footer><div class=container><div class=right><div class=external-profiles><strong>Social media</strong>
<a href=//twitter.com/anoff_io target=_blank><i class="fa fa-twitter-adblock-proof"></i></a><a href=//github.com/anoff target=_blank><i class="fa fa-github"></i></a><a href=//linkedin.com/in/offenhaeuser/ target=_blank><i class="fa fa-linkedin"></i></a></div></div></div></footer><div class=credits><div class=container><div class=copyright><a href=//anoff.io target=_blank>&copy;
2019
by Andreas Offenhaeuser</a>
-
<a href=https://blog.anoff.io/tags/terraform/index.xml>RSS</a></div><div class=author><a href=//github.com/Lednerb/bilberry-hugo-theme target=_blank>Bilberry Hugo Theme</a></div></div></div><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-93890295-4','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><script type=text/javascript src=/js/externalDependencies.39c47e10e241eae2947b3fe21809c572.js integrity="md5-OcR&#43;EOJB6uKUez/iGAnFcg=="></script><script type=text/javascript src=/js/theme.ff50ae6dc1bfc220b23bf69dbb41b54e.js integrity="md5-/1CubcG/wiCyO/adu0G1Tg=="></script><script>$(".moment").each(function(){$(this).text(moment($(this).text()).locale("en").format('LL'));});$(".footnote-return sup").html("");</script><script src=https://blog.anoff.io/fix-adoc.js type=application/javascript></script><script src=//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.0/cookieconsent.min.js type=application/javascript></script><script src=https://blog.anoff.io/init-cookieconsent.js type=application/javascript></script></body></html>